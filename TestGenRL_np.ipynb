{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyCkt import PyCkt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin2dec(bin_nb):\n",
    "    \"\"\"converts binary no string to uint\"\"\"\n",
    "    \n",
    "    return int(bin_nb, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"returns sigmoid of a vector x\"\"\"\n",
    "    \n",
    "    return list(map(lambda z: 1.0/(1.0 + np.exp(-z)), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(vector):\n",
    "    \"\"\"returns relu of a vector\"\"\"\n",
    "    \n",
    "    vector[vector < 0] = 0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(probability):\n",
    "    \"\"\"returns an action given the probability of 1\"\"\"\n",
    "    \n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    \"\"\" In a sequential system the actions you take 20 steps before the end result are more important to the \n",
    "    overall result than an action you took one step ago. Note that gamma gets multiplied the most with the \n",
    "    latest action and least with the first action\"\"\"\n",
    "    \n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    \n",
    "    # running_add is the accumulators of the discounted rewards\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
    "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
    "    \n",
    "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return gradient_log_p * discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(observations):\n",
    "    \"\"\"converts observation from string to list\"\"\"\n",
    "    \n",
    "    ar_ = []\n",
    "    for observation in observations:\n",
    "        for c_ in observation:\n",
    "            if c_ == '1':\n",
    "                ar_.append(1)\n",
    "            else:\n",
    "                ar_.append(0)\n",
    "    return ar_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vec(vlist):\n",
    "    \"\"\"converts vector from list to string\"\"\"\n",
    "    \n",
    "    s = ''\n",
    "    for e in vlist:\n",
    "        if e==1:\n",
    "            s = s + '1'\n",
    "        else:\n",
    "            s = s + '0'\n",
    "    return s.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGen_Rl:\n",
    "    \"\"\"a reinforcement learning based test generator for sequential circuits\"\"\"\n",
    "    \n",
    "    # circuit configuration\n",
    "    coeff = []\n",
    "    intc = 0\n",
    "    sel_indices = []\n",
    "    max_obs = 0\n",
    "    num_p = 1\n",
    "    partition_size = 1\n",
    "    obs_values = []\n",
    "    obs_value_counts = []\n",
    "    ckt = None\n",
    "    init_seq = []\n",
    "    \n",
    "    \n",
    "    # nn hyperparameters\n",
    "    batch_size = 1\n",
    "    gamma = 0.9\n",
    "    decay_rate = 0.99\n",
    "    learning_rate = 1e-4\n",
    "    num_hidden_layers = 1\n",
    "    num_hidden_layer_neurons = {}\n",
    "    weights = {}\n",
    "    veclen = 0\n",
    "    input_dimensions = 0\n",
    "    output_dimensions = 0\n",
    "    reward_sum = 0\n",
    "    prev_reward_sum = 0\n",
    "    observations = []\n",
    "    max_vecs = 0 \n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                cktname,\n",
    "                coef,\n",
    "                intc,\n",
    "                feature_indices,\n",
    "                init_sequence,\n",
    "                nn_configuration,\n",
    "                vpp = 1,\n",
    "                partition_size = 2,\n",
    "                batch_size = 1,\n",
    "                gamma = 0.99,\n",
    "                decay_rate = 0.99,\n",
    "                learning_rate = 1e-4,\n",
    "                max_vecs = 0):\n",
    "        \"\"\"constructor for TestGen_Rl\"\"\"\n",
    "        \n",
    "        self.coeff = coef\n",
    "        self.intrc = intc\n",
    "        self.sel_indices = list(map(lambda x: x - 1, feature_indices))\n",
    "        self.max_obs = 2**partition_size\n",
    "        self.ckt = PyCkt(cktname)\n",
    "        self.vpp = vpp\n",
    "        self.input_dimensions = self.vpp * (self.ckt.getNumPo() + self.ckt.getNumState())\n",
    "        self.veclen = self.ckt.getNumPi()\n",
    "        self.output_dimensions = self.vpp * self.veclen\n",
    "        self.partition_size = partition_size\n",
    "        self.num_p = math.ceil((self.ckt.getNumPo() + self.ckt.getNumState()) / self.partition_size)\n",
    "        self.init_seq = init_sequence\n",
    "        \n",
    "        # lists of all values observed at all windows (or partitions)\n",
    "        self.obs_values = [[] for w in range(self.num_p)]\n",
    "        \n",
    "        # lists of counts of all values observed at all windows\n",
    "        self.obs_value_counts = [[0 for i in range(self.max_obs)] for w in range(self.num_p)]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.decay_rate = decay_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_vecs = max_vecs\n",
    "        \n",
    "        self.num_hidden_layers = len(nn_configuration)\n",
    "        \n",
    "        for layer_nb, layer_size in enumerate(nn_configuration):\n",
    "            self.num_hidden_layer_neurons[layer_nb] = layer_size\n",
    "        \n",
    "        for layer_nb in range(self.num_hidden_layers + 1):\n",
    "            if layer_nb == 0:\n",
    "                self.weights[layer_nb] = np.random.randn(self.num_hidden_layer_neurons[layer_nb], self.input_dimensions)\n",
    "            elif layer_nb ==  self.num_hidden_layers:\n",
    "                self.weights[layer_nb] = np.random.randn(self.output_dimensions, self.num_hidden_layer_neurons[layer_nb-1])\n",
    "            else:\n",
    "                self.weights[layer_nb] = np.random.randn(self.num_hidden_layer_neurons[layer_nb],\n",
    "                                                        self.num_hidden_layer_neurons[layer_nb-1])\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"executes the reinforcement learning algorithm\"\"\"\n",
    "        \n",
    "        pred_fc = []\n",
    "        expectation_g_squared = {}\n",
    "        g_dict = {}\n",
    "        episode_number = 0\n",
    "        episode_hidden_layer_values = {}\n",
    "        for wt_ind in self.weights.keys():\n",
    "            expectation_g_squared[wt_ind] = np.zeros_like(self.weights[wt_ind])\n",
    "            g_dict[wt_ind] = np.zeros_like(self.weights[wt_ind])\n",
    "            episode_hidden_layer_values[wt_ind] = []\n",
    "\n",
    "        episode_observations = []\n",
    "        episode_gradient_log_ps = []\n",
    "        episode_rewards = []\n",
    "        num_applied_vecs = 0\n",
    "        \n",
    "        # initialize ckt\n",
    "        self.observations = []\n",
    "        self.ckt.reset()\n",
    "        for vec in self.init_seq:\n",
    "            resp, state = self.ckt.lsim_s(vec.encode())\n",
    "        self.observations.append((resp + state).decode())\n",
    "        \n",
    "        for vec_ind in range(self.vpp - 1):\n",
    "            #print ('appending %d random vecs to init sequence' % (self.vpp-1))\n",
    "            vec = [np.random.randint(0,1) for _ in range(self.veclen)]\n",
    "            resp, state = self.ckt.lsim_s(encode_vec(vec))\n",
    "            self.observations.append((resp + state).decode())\n",
    "            num_applied_vecs += 1\n",
    "        \n",
    "        print (num_applied_vecs)\n",
    "        self.prev_reward_sum = 0\n",
    "        self.reward_sum = self.get_est_fcov(num_applied_vecs)\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            processed_observations = preprocess_observation(self.observations)\n",
    "            hidden_layer_values, up_probability = self.apply_neural_nets(processed_observations)\n",
    "\n",
    "            episode_observations.append(processed_observations)\n",
    "            for layer_nb in range(self.num_hidden_layers):\n",
    "                episode_hidden_layer_values[layer_nb].append(hidden_layer_values[layer_nb])\n",
    "            \n",
    "            # action\n",
    "            action = list(map(lambda x: choose_action(x), up_probability))\n",
    "            self.observations = []\n",
    "            # carry out the chosen action\n",
    "            for vec_ind in range(self.vpp):\n",
    "                a = vec_ind * self.veclen\n",
    "                b = (vec_ind + 1) * self.veclen\n",
    "                vec = action[a:b]\n",
    "                resp, state = self.ckt.lsim_s(encode_vec(vec))\n",
    "                self.observations.append((resp+state).decode())\n",
    "                num_applied_vecs += 1\n",
    "            #print num_applied_vecs\n",
    "\n",
    "            self.prev_reward_sum = self.reward_sum\n",
    "            self.reward_sum = self.get_est_fcov(num_applied_vecs)\n",
    "            self.reward = self.reward_sum - self.prev_reward_sum\n",
    "            episode_rewards.append(self.reward)\n",
    "\n",
    "            # see here: http://cs231n.github.io/neural-networks-2/#losses\n",
    "            # we cheat for all steps in the episode, for all episodes :st\n",
    "            fake_labels = action\n",
    "            loss_function_gradient = np.subtract(fake_labels, up_probability)\n",
    "            episode_gradient_log_ps.append(loss_function_gradient)\n",
    "\n",
    "            if num_applied_vecs >= self.max_vecs: # an episode finished\n",
    "                episode_number += 1\n",
    "\n",
    "                # convert the lists to numpy arrays for easier processing\n",
    "                for layer_nb in range(self.num_hidden_layers):\n",
    "                    episode_hidden_layer_values[layer_nb] = np.vstack(episode_hidden_layer_values[layer_nb])\n",
    "                episode_observations = np.vstack(episode_observations)\n",
    "                episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "                episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "                # Tweak the gradient of the log_ps based on the discounted rewards\n",
    "                # we have rewards recorded for each step in the episode\n",
    "                episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, self.gamma)\n",
    "\n",
    "                gradient = self.compute_gradient(\n",
    "                  episode_gradient_log_ps_discounted,\n",
    "                  episode_hidden_layer_values,\n",
    "                  episode_observations\n",
    "                )\n",
    "\n",
    "                # Sum the gradient for use when we hit the batch size\n",
    "                for layer_name in gradient:\n",
    "                    g_dict[layer_name] += gradient[layer_name]\n",
    "\n",
    "                if episode_number % self.batch_size == 0:\n",
    "                    self.update_weights(expectation_g_squared, g_dict)\n",
    "\n",
    "                print ('resetting env. episode reward total was %f' % self.reward_sum)\n",
    "                pred_fc.append(self.reward_sum)\n",
    "\n",
    "                episode_hidden_layer_values = {}\n",
    "                for wt_ind in self.weights.keys():\n",
    "                    expectation_g_squared[wt_ind] = np.zeros_like(self.weights[wt_ind])\n",
    "                    g_dict[wt_ind] = np.zeros_like(self.weights[wt_ind])\n",
    "                \n",
    "                for layer_nb in range(self.num_hidden_layers):\n",
    "                    episode_hidden_layer_values[layer_nb] = []\n",
    "\n",
    "                episode_observations = []\n",
    "                episode_gradient_log_ps = []\n",
    "                episode_rewards = []\n",
    "                num_applied_vecs = 0\n",
    "\n",
    "                self.obs_values = [[] for w in range(self.num_p)]\n",
    "                self.obs_value_counts = [[0 for i in range(self.max_obs)] for w in range(self.num_p)]\n",
    "                self.ckt.reset()\n",
    "                \n",
    "                self.observations = []\n",
    "                for vec in self.init_seq:\n",
    "                    resp, state = self.ckt.lsim_s(vec.encode())\n",
    "                self.observations.append((resp + state).decode())\n",
    "                \n",
    "                for vec_ind in range(self.vpp - 1):\n",
    "                    vec = [np.random.randint(0,1) for _ in range(self.veclen)]\n",
    "                    resp, state = self.ckt.lsim_s(encode_vec(vec))\n",
    "                    self.observations.append((resp+state).decode())\n",
    "                    num_applied_vecs += 1\n",
    "            \n",
    "                self.prev_reward_sum = 0\n",
    "                self.reward_sum = self.get_est_fcov(num_applied_vecs)\n",
    "        \n",
    "    def compute_gradient(self, gradient_log_p, hidden_layer_values, observation_values):\n",
    "        \"\"\" See here: http://neuralnetworksanddeeplearning.com/chap2.html\"\"\"\n",
    "        delta_L = gradient_log_p\n",
    "        dC_dw = {}\n",
    "        delta_l = {}\n",
    "        for layer_nb in reversed(range(self.num_hidden_layers + 1)):\n",
    "            if layer_nb == self.num_hidden_layers:\n",
    "                dC_dw[layer_nb] = np.dot(delta_L.T, hidden_layer_values[layer_nb - 1])\n",
    "                delta_l[layer_nb] = relu(np.dot(delta_L, self.weights[layer_nb]))\n",
    "            elif layer_nb == 0:\n",
    "                dC_dw[layer_nb] = np.dot(delta_l[layer_nb + 1].T, observation_values)\n",
    "            else:\n",
    "                dC_dw[layer_nb] = np.dot(delta_l[layer_nb + 1].T, hidden_layer_values[layer_nb - 1])\n",
    "                delta_l[layer_nb] = relu(np.dot(delta_l[layer_nb + 1], self.weights[layer_nb]))\n",
    "        return dC_dw\n",
    "    \n",
    "    def apply_neural_nets(self, observation_matrix):\n",
    "        \"\"\" Based on the observation_matrix and weights, compute the new hidden layer values and the new output layer values\"\"\"\n",
    "        hidden_layer_values = {}\n",
    "        \n",
    "        for layer_nb in range(self.num_hidden_layers + 1):\n",
    "            if layer_nb == 0:\n",
    "                #print ('weights dim: %s' % str(self.weights[layer_nb].shape))\n",
    "                #print ('obs dim: %s' % str(len(observation_matrix)))\n",
    "                hidden_layer_values[layer_nb] = relu(np.dot(self.weights[layer_nb], observation_matrix))\n",
    "            elif layer_nb == self.num_hidden_layers:\n",
    "                output_layer_values = sigmoid(np.dot(self.weights[layer_nb], hidden_layer_values[layer_nb-1]))\n",
    "            else:\n",
    "                hidden_layer_values[layer_nb] = relu(np.dot(self.weights[layer_nb], hidden_layer_values[layer_nb-1]))\n",
    "        return hidden_layer_values, output_layer_values\n",
    "    \n",
    "    def update_weights(self, expectation_g_squared, g_dict):\n",
    "        \"\"\" See here: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\"\"\"\n",
    "        epsilon = 1e-5\n",
    "        for layer_name in self.weights.keys():\n",
    "            g = g_dict[layer_name]\n",
    "            expectation_g_squared[layer_name] = self.decay_rate * expectation_g_squared[layer_name] + \\\n",
    "                (1 - self.decay_rate) * g**2\n",
    "            self.weights[layer_name] += (self.learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
    "            g_dict[layer_name] = np.zeros_like(self.weights[layer_name]) # reset batch gradient buffer\n",
    "            \n",
    "    def update_part_counts(self):\n",
    "        \"\"\"updates number of observed counts for all partititions in the response\"\"\"\n",
    "    \n",
    "        for observation in self.observations:\n",
    "            for i in range(self.num_p):\n",
    "                #extract window substring\n",
    "                substr = observation[ i*self.partition_size : min(len(observation),(i+1)*self.partition_size) ]\n",
    "\n",
    "                #get count index from the observed number in the window (i.e. substring)\n",
    "                c_ind = bin2dec(substr)\n",
    "\n",
    "                #increment observation count of the observed number\n",
    "                self.obs_value_counts[i][c_ind]+=1\n",
    "\n",
    "                #add to observed numbers if not observed before\n",
    "                if c_ind not in self.obs_values[i]:\n",
    "                    self.obs_values[i].append(c_ind)\n",
    "    \n",
    "    def get_updated_metrics(self, resp_ind):\n",
    "        \"\"\"updates the metrics according to the response and reports them\"\"\"\n",
    "    \n",
    "        #tracks count of numbers observed so far in each window\n",
    "        num_obs = [0 for _ in range(self.num_p)] \n",
    "\n",
    "        #tracks cumulative entropy observed so far\n",
    "        cum_ent = [0. for _ in range(self.num_p)]\n",
    "        \n",
    "        self.update_part_counts()\n",
    "\n",
    "        for i in range(self.num_p):\n",
    "            num_obs[i] = len(self.obs_values[i])\n",
    "            ent = 0.\n",
    "            for j in range(self.max_obs):\n",
    "                pj = self.obs_value_counts[i][j] * 1.0 / (resp_ind + 1)\n",
    "                if pj != 0:\n",
    "                     ent = ent + pj * math.log(pj,2)\n",
    "            cum_ent[i] = ent\n",
    "\n",
    "        # concatenate all features together\n",
    "        metrics = num_obs + cum_ent\n",
    "        metrics = list(map(lambda x: abs(x), metrics))\n",
    "\n",
    "        #return selected metrics only\n",
    "        sel_mets = []\n",
    "        for j in range(len(metrics)):\n",
    "            if j in self.sel_indices:\n",
    "                sel_mets.append(metrics[j])\n",
    "\n",
    "        return sel_mets\n",
    "    \n",
    "    def get_est_fcov(self, resp_ind):\n",
    "        \"\"\"returns new estimated fault coverage from circuit outputs\"\"\"\n",
    "    \n",
    "        #get values of selected features\n",
    "        sel_feat = self.get_updated_metrics(resp_ind)\n",
    "        return np.dot(sel_feat, self.coeff) + self.intrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define globals \n",
    "cktname = 's820'\n",
    "coeff = [0.02586818,  0.01588013, -0.16383635, -0.09241966, 0.08055655, 0.0037864, 0.23622088, 0.12172977]\n",
    "intrc = 0.05043711182\n",
    "feature_indices = [11, 12, 15, 18, 19, 20, 23, 24]\n",
    "nn_conf = [200]\n",
    "init_seq = ['010100011011011001','101011010110110110']\n",
    "\n",
    "\n",
    "tg = TestGen_Rl(cktname.encode(),\n",
    "                coeff,\n",
    "                intrc,\n",
    "                feature_indices,\n",
    "                init_seq,\n",
    "                nn_conf,\n",
    "                vpp = 1,\n",
    "                partition_size = 2,\n",
    "                batch_size = 1,\n",
    "                gamma = 0.9,\n",
    "                decay_rate = 0.99,\n",
    "                learning_rate = 5e-4,\n",
    "                max_vecs = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.input_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.output_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.veclen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tg.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "resetting env. episode reward total was 0.198915\n",
      "resetting env. episode reward total was 0.212549\n",
      "resetting env. episode reward total was 0.263701\n",
      "resetting env. episode reward total was 0.187696\n",
      "resetting env. episode reward total was 0.270523\n",
      "resetting env. episode reward total was 0.276156\n",
      "resetting env. episode reward total was 0.288939\n",
      "resetting env. episode reward total was 0.312179\n",
      "resetting env. episode reward total was 0.325990\n",
      "resetting env. episode reward total was 0.327430\n",
      "resetting env. episode reward total was 0.329888\n",
      "resetting env. episode reward total was 0.382526\n",
      "resetting env. episode reward total was 0.426519\n",
      "resetting env. episode reward total was 0.456716\n",
      "resetting env. episode reward total was 0.511935\n",
      "resetting env. episode reward total was 0.556194\n",
      "resetting env. episode reward total was 0.561713\n",
      "resetting env. episode reward total was 0.569094\n",
      "resetting env. episode reward total was 0.598067\n",
      "resetting env. episode reward total was 0.596348\n",
      "resetting env. episode reward total was 0.602848\n",
      "resetting env. episode reward total was 0.625134\n",
      "resetting env. episode reward total was 0.637119\n",
      "resetting env. episode reward total was 0.638747\n",
      "resetting env. episode reward total was 0.641505\n",
      "resetting env. episode reward total was 0.650151\n",
      "resetting env. episode reward total was 0.653830\n",
      "resetting env. episode reward total was 0.626478\n",
      "resetting env. episode reward total was 0.650667\n",
      "resetting env. episode reward total was 0.606004\n",
      "resetting env. episode reward total was 0.641239\n",
      "resetting env. episode reward total was 0.653579\n",
      "resetting env. episode reward total was 0.657838\n",
      "resetting env. episode reward total was 0.643343\n",
      "resetting env. episode reward total was 0.654494\n",
      "resetting env. episode reward total was 0.658488\n",
      "resetting env. episode reward total was 0.658916\n",
      "resetting env. episode reward total was 0.649084\n",
      "resetting env. episode reward total was 0.612708\n",
      "resetting env. episode reward total was 0.641625\n",
      "resetting env. episode reward total was 0.656599\n",
      "resetting env. episode reward total was 0.667489\n",
      "resetting env. episode reward total was 0.659429\n",
      "resetting env. episode reward total was 0.658937\n",
      "resetting env. episode reward total was 0.652108\n",
      "resetting env. episode reward total was 0.656128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b637b987d705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-f541edc9cdff>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlayer_nb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                     \u001b[0mepisode_hidden_layer_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_nb\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_hidden_layer_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_nb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mepisode_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_observations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m                 \u001b[0mepisode_gradient_log_ps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_gradient_log_ps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[0mepisode_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \"\"\"\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tg.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
