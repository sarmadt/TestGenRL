{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyCkt import PyCkt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for measuring fault coverage\n",
    "def bin2dec(bin_nb):\n",
    "    \"\"\"converts binary no string to uint\"\"\"\n",
    "    \n",
    "    return int(bin_nb, 2)\n",
    "    \n",
    "\n",
    "def update_part_counts():\n",
    "    \"\"\"updates number of observed counts for all partititions in the response\"\"\"\n",
    "\n",
    "    global obs_value_counts, obs_values\n",
    "    for observation in observations:\n",
    "        for i in range(num_p):\n",
    "            #extract window substring\n",
    "            substr = observation[ i*partition_size : min(len(observation),\n",
    "                                                         (i+1)*partition_size) ]\n",
    "\n",
    "            #get count index from the observed number in the window (i.e. substring)\n",
    "            c_ind = bin2dec(substr)\n",
    "\n",
    "            #increment observation count of the observed number\n",
    "            obs_value_counts[i][c_ind]+=1\n",
    "\n",
    "            #add to observed numbers if not observed before\n",
    "            if c_ind not in obs_values[i]:\n",
    "                obs_values[i].append(c_ind)\n",
    "\n",
    "def get_updated_metrics(resp_ind):\n",
    "    \"\"\"updates the metrics according to the response and reports them\"\"\"\n",
    "\n",
    "    #tracks count of numbers observed so far in each window\n",
    "    num_obs = [0 for _ in range(num_p)] \n",
    "\n",
    "    #tracks cumulative entropy observed so far\n",
    "    cum_ent = [0. for _ in range(num_p)]\n",
    "\n",
    "    update_part_counts()\n",
    "\n",
    "    for i in range(num_p):\n",
    "        num_obs[i] = len(obs_values[i])\n",
    "        ent = 0.\n",
    "        for j in range(max_obs):\n",
    "            pj = obs_value_counts[i][j] * 1.0 / (resp_ind + 1)\n",
    "            if pj != 0:\n",
    "                 ent = ent + pj * math.log(pj,2)\n",
    "        cum_ent[i] = ent\n",
    "\n",
    "    # concatenate all features together\n",
    "    metrics = num_obs + cum_ent\n",
    "    metrics = list(map(lambda x: abs(x), metrics))\n",
    "\n",
    "    #return selected metrics only\n",
    "    sel_mets = []\n",
    "    for j in range(len(metrics)):\n",
    "        if j in sel_indices:\n",
    "            sel_mets.append(metrics[j])\n",
    "\n",
    "    return sel_mets\n",
    "\n",
    "def get_est_fcov(resp_ind):\n",
    "    \"\"\"returns new estimated fault coverage from circuit outputs\"\"\"\n",
    "\n",
    "    #get values of selected features\n",
    "    sel_feat = get_updated_metrics(resp_ind)\n",
    "    return np.dot(sel_feat, coeff) + intrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface adapters\n",
    "def preprocess_observation(observations):\n",
    "    \"\"\"converts observation from string to list\"\"\"\n",
    "    \n",
    "    ar_ = []\n",
    "    for observation in observations:\n",
    "        for c_ in observation:\n",
    "            if c_ == '1':\n",
    "                ar_.append(1)\n",
    "            else:\n",
    "                ar_.append(0)\n",
    "    return ar_\n",
    "\n",
    "def encode_vec(vlist):\n",
    "    \"\"\"converts vector from list to string\"\"\"\n",
    "    \n",
    "    s = ''\n",
    "    for e in vlist:\n",
    "        if e==1:\n",
    "            s = s + '1'\n",
    "        else:\n",
    "            s = s + '0'\n",
    "    return s.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent helper functions\n",
    "def choose_action(probability):\n",
    "    \"\"\"returns an action given the probability of 1\"\"\"\n",
    "    \n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    \"\"\" In a sequential system the actions you take 20 steps before the end result are more important to the \n",
    "    overall result than an action you took one step ago. Note that gamma gets multiplied the most with the \n",
    "    latest action and least with the first action\"\"\"\n",
    "    \n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    \n",
    "    # running_add is the accumulators of the discounted rewards\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    \n",
    "    discounted_rewards -= np.mean(discounted_rewards)\n",
    "    discounted_rewards /= (np.std(discounted_rewards) + 1e-10)\n",
    "    \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    \n",
    "    def __init__(self, hparams, sess):\n",
    "        \n",
    "        # initialization\n",
    "        self._s = sess\n",
    "        \n",
    "        # build the graph\n",
    "        # forward path\n",
    "        self._input = tf.placeholder(tf.float32,\n",
    "                                    shape = [None, hparams['input_size']])\n",
    "        \n",
    "        hidden = []\n",
    "        for layer_nb in range(len(hparams['nn_conf'])):\n",
    "            if layer_nb == 0:\n",
    "                layer_inp = self._input\n",
    "            else:\n",
    "                layer_inp = hidden[layer_nb-1]\n",
    "            hidden.append(tf.contrib.layers.fully_connected(\n",
    "                    inputs = layer_inp,\n",
    "                    num_outputs = hparams['nn_conf'][layer_nb],\n",
    "                    activation_fn = tf.nn.relu,\n",
    "                    weights_initializer = tf.contrib.layers.xavier_initializer()))\n",
    "\n",
    "        self._up_prob = tf.contrib.layers.fully_connected(\n",
    "                inputs = hidden[-1],\n",
    "                num_outputs = hparams['num_actions'],\n",
    "                activation_fn = tf.nn.softmax)\n",
    "        \n",
    "        \n",
    "        # backward path (training)\n",
    "        tvars = tf.trainable_variables()\n",
    "        self._tvars = tvars\n",
    "        self._actions = tf.placeholder(tf.float32, \n",
    "                        shape = [None, hparams['num_actions']])\n",
    "        self._rewards = tf.placeholder(tf.float32, \n",
    "                        shape = [None, 1])\n",
    "        \n",
    "        log_prob = tf.log(self._up_prob)\n",
    "        log_prob_compl = tf.log(1. - self._up_prob)\n",
    "        \n",
    "        #log_likelihood = self._actions * (self._actions - self._up_prob) + \\\n",
    "        #(1 - self._actions) * (self._actions + self._up_prob)\n",
    "        \n",
    "        step_losses = self._actions * log_prob + \\\n",
    "        (1 - self._actions) * log_prob_compl\n",
    "        #loss = tf.reduce_sum(log_likelihood * self._rewards)\n",
    "        loss = -tf.reduce_sum(step_losses * self._rewards)\n",
    "        self._gradients = tf.gradients(loss, tvars)\n",
    "        \n",
    "        # nn update\n",
    "        #optimizer = tf.train.RMSPropOptimizer(hparams['learning_rate'],\n",
    "        #                                     hparams['decay_rate'])\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(hparams['learning_rate'])\n",
    "        \n",
    "        self._modified_gradients = []\n",
    "        for var_ind in range(len(tvars)):\n",
    "            self._modified_gradients.append(tf.placeholder(tf.float32))\n",
    "            \n",
    "        self._update_grads = optimizer.apply_gradients(zip(self._modified_gradients,\n",
    "                                     tvars))\n",
    "     \n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"get the actions (vector) by sampling\"\"\"\n",
    "        \n",
    "        return self._s.run(self._up_prob,\n",
    "                            feed_dict={self._input: [observation]})\n",
    "    \n",
    "    def get_gradients(self, obs, acts, rewards):\n",
    "        \"\"\"computes and returns the gradients\"\"\"\n",
    "        batch_feed = {self._input: obs,\n",
    "                     self._actions: acts,\n",
    "                     self._rewards: rewards}\n",
    "        return self._s.run(self._gradients, feed_dict = batch_feed)\n",
    "        \n",
    "    def apply_grads(self, gradient_buffer):\n",
    "        \"\"\"batch applies the modified gradients\"\"\"\n",
    "        batch_feed = {}\n",
    "        for ind, item in enumerate(gradient_buffer):\n",
    "            batch_feed[self._modified_gradients[ind]] = item\n",
    "        self._s.run(self._update_grads, feed_dict = batch_feed)\n",
    "        \n",
    "    def get_tvars(self):\n",
    "        return self._tvars\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_rollout(ckt, agent):\n",
    "    \"\"\"run one episode\"\"\"\n",
    "    \n",
    "    # episode interface containers\n",
    "    episode_observations = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # reset the containers that hold the environment measurements internally\n",
    "    num_applied_vecs = 0\n",
    "    global obs_values, obs_value_counts, fcov, max_reward_sum, best_actions\n",
    "    obs_values = [[] for w in range(num_p)]\n",
    "    \n",
    "    obs_value_counts = [[0 for i in range(max_obs)] for w in range(num_p)]\n",
    "    \n",
    "    # initialize ckt\n",
    "    ckt.reset()\n",
    "    global observations\n",
    "    observations = []\n",
    "    for vec in init_seq:\n",
    "        resp, state = ckt.lsim_s(vec.encode())\n",
    "    observations.append((resp + state).decode())\n",
    "\n",
    "    # post apply random vectors to the initialization sequence\n",
    "    # to complete the vector per particle count vpp\n",
    "    for vec_ind in range(vpp - 1):\n",
    "        #print ('appending %d random vecs to init sequence' % (vpp-1))\n",
    "        vec = [np.random.randint(0,1) for _ in range(veclen)]\n",
    "        resp, state = ckt.lsim_s(encode_vec(vec))\n",
    "        observations.append((resp + state).decode())\n",
    "        num_applied_vecs += 1\n",
    "\n",
    "    # we sample cumulative reward after every vpp vectors\n",
    "    prev_reward_sum = 0\n",
    "    reward_sum = get_est_fcov(num_applied_vecs)\n",
    "    \n",
    "    while num_applied_vecs < max_vecs:\n",
    "        # convert list of strings to a flattened list of integers\n",
    "        processed_observations = preprocess_observation(observations)\n",
    "        episode_observations.append(processed_observations)\n",
    "        \n",
    "        # an action is a flattened set of vpp vectors\n",
    "        prob = (agent.act(processed_observations)).ravel()\n",
    "        action = list(map(lambda x: choose_action(x), prob))\n",
    "        episode_actions.append(action)\n",
    "        \n",
    "        observations = []\n",
    "        \n",
    "        # carry out the sampled action i.e.\n",
    "        # extract vpp vectors and apply them to ckt\n",
    "        for vec_ind in range(vpp):\n",
    "            lb = vec_ind * veclen\n",
    "            ub = (vec_ind + 1) * veclen\n",
    "            vec = action[lb:ub]\n",
    "            resp, state = ckt.lsim_s(encode_vec(vec))\n",
    "            observations.append((resp+state).decode())\n",
    "            num_applied_vecs += 1\n",
    "\n",
    "        prev_reward_sum = reward_sum\n",
    "        reward_sum = get_est_fcov(num_applied_vecs)\n",
    "        \n",
    "        \n",
    "        # reward to an action is obtained from cumulative reward\n",
    "        reward = reward_sum - prev_reward_sum\n",
    "        episode_rewards.append(reward)\n",
    "    \n",
    "    print ('episode reward total was %f' % reward_sum)\n",
    "    if reward_sum > max_reward_sum:\n",
    "        max_reward_sum = reward_sum\n",
    "        best_actions = episode_actions\n",
    "    f_cov.append(reward_sum)\n",
    "    # print ('applied vecs %f' % num_applied_vecs)\n",
    "    # return the summary of the episode\n",
    "    return np.vstack(episode_observations), np.vstack(episode_actions), \\\n",
    "    np.vstack(episode_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 4\n",
      "episode reward total was 0.360034\n",
      "episode reward total was 0.386981\n",
      "episode reward total was 0.371297\n",
      "episode reward total was 0.381237\n",
      "episode reward total was 0.393319\n",
      "episode reward total was 0.382590\n",
      "episode reward total was 0.381702\n",
      "episode reward total was 0.381906\n",
      "episode reward total was 0.393652\n",
      "episode reward total was 0.372615\n",
      "episode reward total was 0.374328\n",
      "episode reward total was 0.392733\n",
      "episode reward total was 0.373352\n",
      "episode reward total was 0.386388\n",
      "episode reward total was 0.404622\n",
      "episode reward total was 0.379067\n",
      "episode reward total was 0.390186\n",
      "episode reward total was 0.389051\n",
      "episode reward total was 0.405400\n",
      "episode reward total was 0.396341\n",
      "episode reward total was 0.395210\n",
      "episode reward total was 0.415123\n",
      "episode reward total was 0.402318\n",
      "episode reward total was 0.400709\n",
      "episode reward total was 0.384952\n",
      "episode reward total was 0.414015\n",
      "episode reward total was 0.391562\n",
      "episode reward total was 0.398636\n",
      "episode reward total was 0.418315\n",
      "episode reward total was 0.388604\n",
      "episode reward total was 0.411015\n",
      "episode reward total was 0.407465\n",
      "episode reward total was 0.403069\n",
      "episode reward total was 0.410882\n",
      "episode reward total was 0.405677\n",
      "episode reward total was 0.428434\n",
      "episode reward total was 0.409423\n",
      "episode reward total was 0.425705\n",
      "episode reward total was 0.411091\n",
      "episode reward total was 0.422275\n",
      "episode reward total was 0.399048\n",
      "episode reward total was 0.423541\n",
      "episode reward total was 0.419583\n",
      "episode reward total was 0.414521\n",
      "episode reward total was 0.408099\n",
      "episode reward total was 0.428102\n",
      "episode reward total was 0.406555\n",
      "episode reward total was 0.415116\n",
      "episode reward total was 0.416153\n",
      "episode reward total was 0.431095\n",
      "episode reward total was 0.429210\n",
      "episode reward total was 0.410454\n",
      "episode reward total was 0.424223\n",
      "episode reward total was 0.445047\n",
      "episode reward total was 0.436352\n",
      "episode reward total was 0.433525\n",
      "episode reward total was 0.425322\n",
      "episode reward total was 0.421427\n",
      "episode reward total was 0.442399\n",
      "episode reward total was 0.436341\n",
      "episode reward total was 0.440946\n",
      "episode reward total was 0.414020\n",
      "episode reward total was 0.434149\n",
      "episode reward total was 0.418654\n",
      "episode reward total was 0.429311\n",
      "episode reward total was 0.449416\n",
      "episode reward total was 0.450184\n",
      "episode reward total was 0.449923\n",
      "episode reward total was 0.442456\n",
      "episode reward total was 0.448811\n",
      "episode reward total was 0.439546\n",
      "episode reward total was 0.432528\n",
      "episode reward total was 0.470853\n",
      "episode reward total was 0.473739\n",
      "episode reward total was 0.451447\n",
      "episode reward total was 0.454936\n",
      "episode reward total was 0.475298\n",
      "episode reward total was 0.471058\n",
      "episode reward total was 0.472843\n",
      "episode reward total was 0.457821\n",
      "episode reward total was 0.490780\n",
      "episode reward total was 0.465539\n",
      "episode reward total was 0.473260\n",
      "episode reward total was 0.477545\n",
      "episode reward total was 0.477318\n",
      "episode reward total was 0.482038\n",
      "episode reward total was 0.487145\n",
      "episode reward total was 0.486818\n",
      "episode reward total was 0.496437\n",
      "episode reward total was 0.503058\n",
      "episode reward total was 0.488908\n",
      "episode reward total was 0.469729\n",
      "episode reward total was 0.478735\n",
      "episode reward total was 0.503580\n",
      "episode reward total was 0.499282\n",
      "episode reward total was 0.498055\n",
      "episode reward total was 0.497486\n",
      "episode reward total was 0.524013\n",
      "episode reward total was 0.514897\n",
      "episode reward total was 0.495909\n",
      "episode reward total was 0.493166\n",
      "episode reward total was 0.489491\n",
      "episode reward total was 0.515759\n",
      "episode reward total was 0.509756\n",
      "episode reward total was 0.490134\n",
      "episode reward total was 0.518882\n",
      "episode reward total was 0.511615\n",
      "episode reward total was 0.536384\n",
      "episode reward total was 0.493037\n",
      "episode reward total was 0.531941\n",
      "episode reward total was 0.514124\n",
      "episode reward total was 0.518120\n",
      "episode reward total was 0.529024\n",
      "episode reward total was 0.528623\n",
      "episode reward total was 0.514479\n",
      "episode reward total was 0.526493\n",
      "episode reward total was 0.543007\n",
      "episode reward total was 0.537386\n",
      "episode reward total was 0.541031\n",
      "episode reward total was 0.513843\n",
      "episode reward total was 0.514991\n",
      "episode reward total was 0.532743\n",
      "episode reward total was 0.509610\n",
      "episode reward total was 0.531852\n",
      "episode reward total was 0.530374\n",
      "episode reward total was 0.497822\n",
      "episode reward total was 0.511479\n",
      "episode reward total was 0.523464\n",
      "episode reward total was 0.537543\n",
      "episode reward total was 0.530013\n",
      "episode reward total was 0.508342\n",
      "episode reward total was 0.519843\n",
      "episode reward total was 0.526451\n",
      "episode reward total was 0.549100\n",
      "episode reward total was 0.542202\n",
      "episode reward total was 0.536830\n",
      "episode reward total was 0.526701\n",
      "episode reward total was 0.547574\n",
      "episode reward total was 0.536745\n",
      "episode reward total was 0.520497\n",
      "episode reward total was 0.519406\n",
      "episode reward total was 0.516914\n",
      "episode reward total was 0.521601\n",
      "episode reward total was 0.544026\n",
      "episode reward total was 0.505384\n",
      "episode reward total was 0.542107\n",
      "episode reward total was 0.533355\n",
      "episode reward total was 0.519786\n",
      "episode reward total was 0.525670\n",
      "episode reward total was 0.532910\n",
      "episode reward total was 0.525570\n",
      "episode reward total was 0.514908\n",
      "episode reward total was 0.537524\n",
      "episode reward total was 0.500164\n",
      "episode reward total was 0.522913\n",
      "episode reward total was 0.503608\n",
      "episode reward total was 0.525709\n",
      "episode reward total was 0.515249\n",
      "episode reward total was 0.528981\n",
      "episode reward total was 0.519252\n",
      "episode reward total was 0.512356\n",
      "episode reward total was 0.511697\n",
      "episode reward total was 0.530065\n",
      "episode reward total was 0.547323\n",
      "episode reward total was 0.516416\n",
      "episode reward total was 0.508335\n",
      "episode reward total was 0.527158\n",
      "episode reward total was 0.547696\n",
      "episode reward total was 0.536001\n",
      "episode reward total was 0.534104\n",
      "episode reward total was 0.510046\n",
      "episode reward total was 0.533781\n",
      "episode reward total was 0.529850\n",
      "episode reward total was 0.541519\n",
      "episode reward total was 0.542305\n",
      "episode reward total was 0.531000\n",
      "episode reward total was 0.517165\n",
      "episode reward total was 0.526884\n",
      "episode reward total was 0.551273\n",
      "episode reward total was 0.533174\n",
      "episode reward total was 0.525005\n",
      "episode reward total was 0.528976\n",
      "episode reward total was 0.523634\n",
      "episode reward total was 0.540827\n",
      "episode reward total was 0.546066\n",
      "episode reward total was 0.524134\n",
      "episode reward total was 0.511176\n",
      "episode reward total was 0.529305\n",
      "episode reward total was 0.525841\n",
      "episode reward total was 0.533068\n",
      "episode reward total was 0.522859\n",
      "episode reward total was 0.530717\n",
      "episode reward total was 0.532962\n",
      "episode reward total was 0.531171\n",
      "episode reward total was 0.548765\n",
      "episode reward total was 0.538526\n",
      "episode reward total was 0.526584\n",
      "episode reward total was 0.535551\n",
      "episode reward total was 0.543759\n",
      "episode reward total was 0.538011\n",
      "episode reward total was 0.535998\n",
      "episode reward total was 0.548803\n",
      "episode reward total was 0.532598\n",
      "episode reward total was 0.552224\n",
      "episode reward total was 0.580773\n",
      "episode reward total was 0.549031\n",
      "episode reward total was 0.565141\n",
      "episode reward total was 0.569323\n",
      "episode reward total was 0.557073\n",
      "episode reward total was 0.564983\n",
      "episode reward total was 0.562819\n",
      "episode reward total was 0.570957\n",
      "episode reward total was 0.574697\n",
      "episode reward total was 0.563924\n",
      "episode reward total was 0.562585\n",
      "episode reward total was 0.577836\n",
      "episode reward total was 0.569042\n",
      "episode reward total was 0.581656\n",
      "episode reward total was 0.595195\n",
      "episode reward total was 0.568401\n",
      "episode reward total was 0.587199\n",
      "episode reward total was 0.586024\n",
      "episode reward total was 0.596830\n",
      "episode reward total was 0.593795\n",
      "episode reward total was 0.588810\n",
      "episode reward total was 0.596967\n",
      "episode reward total was 0.595961\n",
      "episode reward total was 0.601639\n",
      "episode reward total was 0.593899\n",
      "episode reward total was 0.595069\n",
      "episode reward total was 0.597815\n",
      "episode reward total was 0.615925\n",
      "episode reward total was 0.614531\n",
      "episode reward total was 0.610959\n",
      "episode reward total was 0.610982\n",
      "episode reward total was 0.615906\n",
      "episode reward total was 0.611565\n",
      "episode reward total was 0.620597\n",
      "episode reward total was 0.623258\n",
      "episode reward total was 0.627414\n",
      "episode reward total was 0.624082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward total was 0.628297\n",
      "episode reward total was 0.623612\n",
      "episode reward total was 0.626316\n",
      "episode reward total was 0.625379\n",
      "episode reward total was 0.633328\n",
      "episode reward total was 0.634289\n",
      "episode reward total was 0.631791\n",
      "episode reward total was 0.632978\n",
      "episode reward total was 0.630644\n",
      "episode reward total was 0.636093\n",
      "episode reward total was 0.635131\n",
      "episode reward total was 0.633266\n",
      "episode reward total was 0.634644\n",
      "episode reward total was 0.633708\n",
      "episode reward total was 0.635903\n",
      "episode reward total was 0.634737\n",
      "episode reward total was 0.633140\n",
      "episode reward total was 0.634880\n",
      "episode reward total was 0.631058\n",
      "episode reward total was 0.633553\n",
      "episode reward total was 0.635072\n",
      "episode reward total was 0.633992\n",
      "episode reward total was 0.632259\n",
      "episode reward total was 0.634532\n",
      "episode reward total was 0.633910\n",
      "episode reward total was 0.634227\n",
      "episode reward total was 0.633923\n",
      "episode reward total was 0.632960\n",
      "episode reward total was 0.634114\n",
      "episode reward total was 0.630886\n",
      "episode reward total was 0.634796\n",
      "episode reward total was 0.632420\n",
      "episode reward total was 0.631766\n",
      "episode reward total was 0.634244\n",
      "episode reward total was 0.632506\n",
      "episode reward total was 0.631092\n",
      "episode reward total was 0.634041\n",
      "episode reward total was 0.632978\n",
      "episode reward total was 0.633729\n",
      "episode reward total was 0.632940\n",
      "episode reward total was 0.634329\n",
      "episode reward total was 0.632665\n",
      "episode reward total was 0.630798\n",
      "episode reward total was 0.631521\n",
      "episode reward total was 0.632863\n",
      "episode reward total was 0.633196\n",
      "episode reward total was 0.632669\n",
      "episode reward total was 0.632588\n",
      "episode reward total was 0.633988\n",
      "episode reward total was 0.631870\n",
      "episode reward total was 0.631260\n",
      "episode reward total was 0.632663\n",
      "episode reward total was 0.631870\n",
      "episode reward total was 0.631913\n",
      "episode reward total was 0.632076\n",
      "episode reward total was 0.632076\n",
      "episode reward total was 0.631913\n",
      "episode reward total was 0.633008\n",
      "episode reward total was 0.631408\n",
      "episode reward total was 0.631288\n",
      "episode reward total was 0.631408\n",
      "episode reward total was 0.632302\n",
      "episode reward total was 0.628983\n",
      "episode reward total was 0.631913\n",
      "episode reward total was 0.630470\n",
      "episode reward total was 0.632619\n",
      "episode reward total was 0.633519\n",
      "episode reward total was 0.629930\n",
      "episode reward total was 0.631788\n",
      "episode reward total was 0.630685\n",
      "episode reward total was 0.631341\n",
      "episode reward total was 0.632327\n",
      "episode reward total was 0.632123\n",
      "episode reward total was 0.631891\n",
      "episode reward total was 0.632523\n",
      "episode reward total was 0.631750\n",
      "episode reward total was 0.632327\n",
      "episode reward total was 0.632411\n",
      "episode reward total was 0.631687\n",
      "episode reward total was 0.632327\n",
      "episode reward total was 0.631750\n",
      "episode reward total was 0.631891\n",
      "episode reward total was 0.631341\n",
      "episode reward total was 0.631891\n",
      "episode reward total was 0.631306\n",
      "episode reward total was 0.632327\n",
      "episode reward total was 0.632411\n",
      "episode reward total was 0.630825\n",
      "episode reward total was 0.631306\n",
      "episode reward total was 0.631687\n",
      "episode reward total was 0.632712\n",
      "episode reward total was 0.631891\n",
      "episode reward total was 0.631891\n",
      "episode reward total was 0.631891\n",
      "episode reward total was 0.631687\n",
      "episode reward total was 0.631543\n",
      "episode reward total was 0.631543\n",
      "episode reward total was 0.630279\n",
      "episode reward total was 0.631891\n",
      "episode reward total was 0.632231\n",
      "episode reward total was 0.630953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f4db8b72b6d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ep_per_batch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_rollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mrews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscount_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-8af053674aa3>\u001b[0m in \u001b[0;36mpolicy_rollout\u001b[1;34m(ckt, agent)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# an action is a flattened set of vpp vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_observations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mepisode_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-8e2d4cf7ed4f>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         return self._s.run(self._up_prob,\n\u001b[1;32m---> 68\u001b[1;33m                             feed_dict={self._input: [observation]})\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define circuit parameters\n",
    "f_cov = []\n",
    "cktname = 's820'\n",
    "coeff = [0.02586818,  0.01588013, -0.16383635, -0.09241966, 0.08055655, 0.0037864, 0.23622088, 0.12172977]\n",
    "intrc = 0.05043711182\n",
    "feature_indices = [11, 12, 15, 18, 19, 20, 23, 24]\n",
    "sel_indices = list(map(lambda x: x - 1, feature_indices))\n",
    "init_seq = ['010100011011011001','101011010110110110']\n",
    "partition_size = 2\n",
    "max_obs = 2**partition_size\n",
    "max_vecs = 700\n",
    "vpp = 1\n",
    "observations = []\n",
    "\n",
    "# define agent parameters\n",
    "nn_conf = [200]\n",
    "gamma = 0.8\n",
    "num_batches = 10000\n",
    "episodes_per_batch = 1\n",
    "learning_rate = 5e-4\n",
    "decay_rate = 0.99\n",
    "\n",
    "# create circuit instance and get internal info\n",
    "ckt = PyCkt(cktname.encode())\n",
    "input_dimensions = vpp * (ckt.getNumPo() + ckt.getNumState())\n",
    "veclen = ckt.getNumPi()\n",
    "statelen = ckt.getNumPo() + ckt.getNumState()\n",
    "output_dimensions = vpp * veclen\n",
    "num_p = math.ceil(statelen / partition_size)\n",
    "max_reward_sum = 0\n",
    "best_actions = []\n",
    "\n",
    "# hyperparameters\n",
    "hparams = {\n",
    "    'input_size': input_dimensions,\n",
    "    'nn_conf': nn_conf,\n",
    "    'num_actions': output_dimensions,\n",
    "    'learning_rate': learning_rate,\n",
    "    'decay_rate': decay_rate\n",
    "}\n",
    "\n",
    "# environment parameters\n",
    "eparams = {\n",
    "    'num_batches': num_batches,\n",
    "    'ep_per_batch': episodes_per_batch\n",
    "}\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    agent = PolicyGradientAgent(hparams, sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_dict = sess.run(agent.get_tvars())\n",
    "    #print (g_dict)\n",
    "    for ind, grad in enumerate(g_dict):\n",
    "        g_dict[ind] = grad * 0.\n",
    "    print (type(g_dict), len(g_dict))\n",
    "    \n",
    "    for batch in range(eparams['num_batches']):\n",
    "     \n",
    "        for _ in range(eparams['ep_per_batch']):\n",
    "            \n",
    "            obs, acts, rews = policy_rollout(ckt, agent)\n",
    "            rews = discount_rewards(rews, gamma)\n",
    "            grads = agent.get_gradients(obs, acts, rews)\n",
    "            \n",
    "            for ind, grad in enumerate(grads):\n",
    "                g_dict[ind] += grad\n",
    "        \n",
    "        agent.apply_grads(g_dict)\n",
    "    print (max_reward_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e215ffcc88>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e198e581d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
